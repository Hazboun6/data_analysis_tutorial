{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>0 Acknowledgements </h2>\n",
    "This notebook is a slightly modified version of the data analyis worksheet developed by Justin Ellis, Rutger van Haasteren, Steve Taylor and others for student workshops run by the Pulsar Timing Array collaboration NANOGrav.\n",
    "\n",
    "<h1>1 Data analysis with sinusoid of unknown amplitude in known Gaussian noise</h1>\n",
    "\n",
    "<p class=lead>\n",
    "    The problem you are faced with is detecting and characterizing a sinusoid\n",
    "    in Gaussian noise.  This is a standard \n",
    "    and simple problem in data analysis that will give you a flavor of the data\n",
    "    analysis problems and techniques used in real GW data analysis. In fact, \n",
    "    nearly the entire GW detection problem is framed in this way: Is there a\n",
    "    detectable GW (be it stochastic, continuous, or burst) present in the puslar\n",
    "    timing residual data? The challenge is coming up with robust and efficient \n",
    "    analysis methods that take into account all of the intracacies of real gravitational\n",
    "    wave timing data.\n",
    "    <br/><br/>\n",
    "    Dealing with real or simulated GW data will only cause additional complications\n",
    "    and will deter you from learning the basics of data analysis. In this activity \n",
    "    you will explore both frequentist and Bayesian techniques for parameter estimation, \n",
    "    detection, and setting upper limits. <br/><br/>\n",
    "    We assume that the data $d$ is composed of a signal $s$ plus additive white gaussian \n",
    "    noise $n$, that is\n",
    "    $$\n",
    "    d=s+n.\n",
    "    $$\n",
    "    Since $n$ is assumed to be white and Gaussian, its probability distribution is a product of\n",
    "    Gaussians\n",
    "    $$\n",
    "    p(n)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{n_i^2}{2\\sigma^2}\\right),\n",
    "    $$\n",
    "    where $n_i$ is the noise at time $t_i$ and $\\sigma$ is the standard deviation of the noise. \n",
    "    We can now write this in terms of the data using $n=d-s$, therefore the likelihood function\n",
    "    for the data given the signal parameters, $\\lambda$ is\n",
    "    $$\n",
    "    p(d|\\lambda)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{(d_i-s_i(\\lambda))^2}{2\\sigma^2}\\right).\n",
    "    $$\n",
    "    For our purposes we will deal with the natural log of the likelihood ratio. The likelihood ratio is the \n",
    "    likelihood function above divided by the likelihood that the signal is pure noise (i.e, remove\n",
    "    the signal $s$ from the model). We will also introduce a new notation and write the log-likelihood \n",
    "    ratio\n",
    "    $$\n",
    "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s),\n",
    "    $$\n",
    "    where the inner product of two timeseries $x$ and $y$, for example, is $(x|y)=\\sum_i x_iy_i/\\sigma^2$.\n",
    "    In our case the signal $s$ is just a sinusoid, $s(t,A,f) = A\\sin(2\\pi ft)$. For simplicity we will \n",
    "    assume that we know the frequency (or that we are doing a \"target\" search for a certain frequency) \n",
    "    of the sinusoid and the only unknown parameter is the amplitude.\n",
    "    <br/><br/>\n",
    "    Your first exercise will be to create a simulated dataset with white noise and a small sinusoid. \n",
    "    <br/><br/>\n",
    "    Good luck!\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import worksheet_utils as wu\n",
    "from scipy.interpolate import interp1d\n",
    "import simple_mcmc as smcmc\n",
    "from corner import corner\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.0 Derving the log-likelihod ratio</h2>\n",
    "\n",
    "<p class='lead'>\n",
    "    Above we have written down the log of the likehood ratio which will be used throughout this \n",
    "    entire worksheet. To get yourself familiar with the notation, derive the log-likelihood ratio\n",
    "    defined above. \n",
    "    <br/><br/>\n",
    "    <em>Hint</em>: It will be easier to take the logarithm of each likelihood function first and\n",
    "    then take the difference (i.e., $\\log(A/B)-\\log(A)-\\log(B)$).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.1 Simulating the data set. </h2>\n",
    "\n",
    "<p class=lead>\n",
    "    While a sinusoid seems exceedingly simple, they are fairly accurate models for monochromatic GW from binaries.\n",
    "    Create a 1 hour timeseries of 150 points using `numpy` function <strong>`linspace()`</strong> beginning at 0. \n",
    "    Then, use the <strong>`simData()`</strong> function from `worksheet_utils` to simulate some fake data with a signal with amplitude $7\\times10^{-20}$ in characteristic strain and frequency of $10^{-3}$ Hz where the standard deviation of the data is $10^{-19}$ in units of characteristic strain.\n",
    "    <br/><br/>\n",
    "    Then use the `matplotlib` function __`plot()`__ to plot the data vs. time and be sure to label the axes. Next, create a separate dataset with no noise by calling the __`signal()`__ function from `worksheet_utils` in order to plot just the signal on top of the signal+noise dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define frequency, Amplitude, timeseries and sigma\n",
    "\n",
    "# call function to simulate data\n",
    "\n",
    "# call wu.signal function to simulate sine wave individually\n",
    "\n",
    "# plot data in blue with signal in red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2 Parameter Estimation: Estimating the Amplitude</h2>\n",
    "\n",
    "<p class=lead>\n",
    "    In frequentist statistics, one is interested in defining some estimator for the true signal parameters.\n",
    "    A common way to do this is to find the parameters that maximize the likelihood. These are known as Maximum\n",
    "    Likelihood Estimators (MLEs). One can then place confidence intervals on the parameter of interest through the \n",
    "    <em>Neyman method</em>: define some statistic $x$ that is a function of the data $d$ (for example the MLE of the\n",
    "    unknown signal parameters). Because $x$ is derived from $d$, the probability distribution (likelihood) $p(d|\\lambda)$\n",
    "    can be re-expressed as a probability distribution $p(x|\\lambda)$. Then, for each value of $\\lambda$, we produce an \n",
    "    interval $(x_1,x_2)$ such that\n",
    "    $$\n",
    "    \\alpha = \\int_{x_1}^{x_2}dx\\,\\, p(x|\\lambda)\n",
    "    $$\n",
    "    The intervals that depend on $\\lambda$ define a belt in the $\\lambda-x$ plane and for any <em>observed</em> value\n",
    "    of $x$ in the experiment, the confidence interval on $\\lambda$ constitutes the values of $\\lambda$ that exist on the \n",
    "    belt at that fixed value of $x$.\n",
    "    <br/><br/>\n",
    "    In Bayesian statistics, one is interested finding the probability distribution function for the unknown parameters given\n",
    "    that we have some observed data. To get a point estimate of the unkwnown parameters we may use the <em>maximum a-posteriori</em>\n",
    "    (MAP) parameters, that is, the maximum of the posterior probability distribution. A credible interval (Bayesian version\n",
    "    of confidence intervals) can be constructed directly from the posterior distribution $p(\\lambda|d)$ as\n",
    "    $$\n",
    "    \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d),\n",
    "    $$\n",
    "    where $\\lambda_1$ and $\\lambda_2$ defined the lower and upper bounds on our parameter $\\lambda$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.1 MLE for the amplitude</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As written above, the log-likelihood ratio is \n",
    "    $$\n",
    "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s).\n",
    "    $$\n",
    "    If we let $s(t)=A\\sin(2\\pi f t)$ and $\\tilde{s}(t)=\\sin(2\\pi f t)$, then $s(t)=A\\tilde{s}(t)$. \n",
    "    Re-writing the log-likehood ratio, we obtain\n",
    "    $$\n",
    "    \\log\\Lambda=A(d|\\tilde{s})-\\frac{A^2}{2}(\\tilde{s}|\\tilde{s}).\n",
    "    $$\n",
    "    We can now maximize the likehood function over the unknown amplitude, $A$, as follows\n",
    "    $$\n",
    "    0 = \\frac{\\partial \\log\\Lambda}{\\partial A} = (d|\\tilde{s}) -\\hat{A}(\\tilde{s}|\\tilde{s}),\n",
    "    $$\n",
    "    where $\\hat{A}$ is the MLE for $A$. Solving for $\\hat{A}$, we obtain\n",
    "    $$\n",
    "    \\hat{A}=\\frac{(d|\\tilde{s})}{(\\tilde{s}|\\tilde{s})}.\n",
    "    $$\n",
    "    The variance on the maximum likelihood estimator is then\n",
    "    $$\n",
    "    \\sigma_{\\hat{A}}^2 = \\langle \\hat{A}\\hat{A} \\rangle -\\langle \\hat{A} \\rangle^2 = \\frac{1}{(\\tilde{s}|\\tilde{s})},\n",
    "    $$\n",
    "    where $\\langle  \\rangle$ denotes the <em>expectation value</em> or average over many realizations of data.\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Write a function that reads in the data, the time samples, the frequency of the sine wave and the \n",
    "        standard deviation of the noise as arguments and outputs the MLE and standard deviation of $A$. \n",
    "        <em>Hint:</em> Look at the `worksheet_utils` package to figure out what functions you may need.<br/>\n",
    "        <span style=\"color:red\">For intermediate/expert level: Derive the variance of $\\hat{A}$. <em>Hint:</em> \n",
    "        $\\langle (n|\\tilde{s}) \\rangle=0$ and $\\langle (n|\\tilde{s})(n|\\tilde{s}) \\rangle=(\\tilde{s}|\\tilde{s})$.</span>\n",
    "    </li>\n",
    "    <li>\n",
    "        Use this function to compute $\\hat{A}$ for our simulated data set.\n",
    "    </li>   \n",
    "    <li>Does the answer you get make sense? What is the percent error?</li>\n",
    "</ol>\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  maximum likelihood estimator for A. fill in function\n",
    "def maxLikeA(data, t, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call maximum likelihood function and print MLE along with the true value of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.2 Posterior distribution for the amplitude</h3>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Write a function that reads in the data, the time samples, the amplitude of the sine wave, the frequency of the sine wave\n",
    "        and the standard deviation of the noise and returns the log-likelihood ratio.\n",
    "    </li>\n",
    "    <li>\n",
    "        Create a vector of trial amplitudes in the range $[0, 2\\times 10^{-19}]$ with length 1000 using\n",
    "        the __`np.linspace()`__ function. You can play around with the length of the array and the ranges\n",
    "        once you have done this once.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Loop over these amplitudes and compute the log-posterior for each one and store\n",
    "        it in an array. Here we will use a uniform prior on $A$ which means that the likelihood\n",
    "        and the posterior are identical except for a normalizing constant which is not important \n",
    "        here. The posterior is then\n",
    "        $$\n",
    "        \\log\\,p(A|d) \\propto \\log\\Lambda(d|A) + \\log p(A),\n",
    "        $$\n",
    "        where p(A) is the prior. \n",
    "    </li>\n",
    "    <li>\n",
    "        This prior must be normalized to 1, so if our prior is uniform in $A$ then\n",
    "        it is the same for every value of $A$, thus $p(A)=C$, where $C$ is a constant. \n",
    "        Integrate this uniform prior to determine\n",
    "        the normalization constant (i.e., use $\\int p(A)dA=1$ to find the value of $C$).\n",
    "    </li>   \n",
    "    <li>\n",
    "        Use the __`plt.plot()`__ function to plot the posterior vs. the \n",
    "        Amplitude Array.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Determine the MAP value of $A$ by using the __`np.argmax()`__ function to find the array index\n",
    "        of the maximum posterior value. You can then use this array index to find the corresponding\n",
    "        value of $A$ from the vector of trial amplitudes that you created in step 2.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Determine the standard deviation of $A$. <em>Hint:</em> recall that the statistical definition\n",
    "        of variance on a parameter $x$ whose pdf is $p(x)$ is $\\sigma^2_x=\\langle x^2 \\rangle - \\langle x \\rangle^2 \n",
    "        = \\int dx \\, x^2\\,p(x) - \\left(\\int dx\\, x\\, p(x)\\right)^2$\n",
    "    </li>    \n",
    "    <li>\n",
    "        Plot the likelihood vs $A$. Plot a vertical line denoting the true value of $A$ with the __`plt.axvline()`__ \n",
    "        function. Also, use the __`plt.axvspan()`__ with the <em>alpha</em> argument to plot a shaded region denoting\n",
    "        the standard deviation of $A$.\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-likelihood ratio function (fill in function)\n",
    "def LogLikelihood(data, t, A, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create amplitude vector and initialize log likelihood vector\n",
    "\n",
    "# loop over amplitude value and call log likelihood function for each amplitude\n",
    "\n",
    "# exponentiate log-likelihood and multiply by prior to get the posterior distribution\n",
    "\n",
    "# use argmax function to find index of maximum value of the posterior. Use array index to find MAP value of A\n",
    "\n",
    "# print MAP value of A and true value\n",
    "\n",
    "# plot posterior vs A in blue and vertical line at the injected value in red\n",
    "\n",
    "# use axhspan to make shaded region denoting 1 standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2.2.1: Gaussian prior distribution on $A$</h4>\n",
    "\n",
    "<p class=lead>\n",
    "    It is often the case in data analysis where we have some additional prior information about our problem before\n",
    "    conducting the experiment. In the Bayesian framework we incorporate this additional information into the prior\n",
    "    probability distribution. The data (via the likelihood function) is then used to update our prior knowledge. If\n",
    "    our data is <em>informative</em> then the posterior distribution will be different then the prior distribution. \n",
    "    On the other hand, if our posterior results in distribution that is identical to the prior then our data is not\n",
    "    informative.\n",
    "    <br/><br/>\n",
    "    To illustrate this, we will assume that through some other experiment, or from some theoretical prediction, we have\n",
    "    some prior knowledge of the amplitude of the sine wave in our data. We will model this as a gaussian prior with the\n",
    "    mean the true value that we used to create the data and some standard deviation\n",
    "    $$\n",
    "    p(A)=\\frac{1}{\\sqrt{2\\pi\\sigma_A^2}}\\exp\\left(-\\frac{(A-A_{\\rm true})^2}{2\\sigma_A^2}\\right)\n",
    "    $$\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Let the fractional uncertainty on $A$ be 25%, that is $\\sigma_A=0.25\\times A$. Use this information in the\n",
    "        gaussian prior and repeat the steps from exercise 2.2. You will need to create an extra array to store the \n",
    "        prior function when conducting the for loop. When constructing the posterior, remember to  multiply the \n",
    "        likelihood by the prior.\n",
    "    </li>\n",
    "    <li>\n",
    "        Make sure that the posterior and prior both are both normalized and plot the posterior and the prior vs. A.\n",
    "        Is the data informative?\n",
    "    </li>\n",
    "    <li>\n",
    "        Repeat this exercise for $\\sigma_A=0.05\\times A$.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat above exercise with the gaussian prior shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.3 Frequentist Confidence Interval on Amplitude Parameter</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As we show above, contructing a frequentist confidence interval involves simulating several realizations of data with different values of the sinusoid amplitude. Here we will construct a function to do just that.\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Create a vector of trial amplitudes in the range $[1\\times 10^{-20}, 3\\times 10^{-19}]$ with length 100 using\n",
    "        the __`linspace()`__ function.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Loop over these amplitudes as above but now add a second inner loop to simulate different <em>\n",
    "        realizations</em> of the data. Use __`simData()`__ function to simulate the new data. For each\n",
    "        amplitude do 1000 realizations of data with that amplitude. (See code block below for hints)\n",
    "    </li>    \n",
    "    <li>\n",
    "        The goal here is to compute the MLE for $A$ for each realization of data. For a given amplitude,\n",
    "        compute $\\hat{A}$ for each data realization using your function from exercise 2.1 and store it in an\n",
    "        array. Then use the __`confinterval()`__ function from `worksheet_utils` to define the upper and lower 1-sigma bounds on the\n",
    "        distribution of $\\hat{A}$ for a given value of $A$. Store these 1-sigma lower and upper\n",
    "        bounds in arrays. This will create a band through the $A$-$\\hat{A}$ space. (See code block below for hints)\n",
    "    </li>    \n",
    "    <li>\n",
    "        After you have looped over all values of the amplitude. Use the <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html\">__`interp1d()`__</a> function to\n",
    "        interpolate the 1-sigma upper and lower bounds on $\\hat{A}$ for each value of $A$.\n",
    "        This way we create an interpolating function so that we can evaluate the\n",
    "        confidence interval on $A$ from our confidence belt and our measurement of $\\hat{A}$. \n",
    "    </li> \n",
    "    <li>\n",
    "        Plot the confidence band (i.e., the injected amplitude vs the 1-sigma upper and lower bounds on $\\hat{A}$.) as\n",
    "        well as a vertical line denoting the measured value of $\\hat{A}$ and a shaded area using the __`plt.axhspan()`__\n",
    "        function to denote the confidence interval on $A$.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of amplitudes and create amplitude vector\n",
    "\n",
    "# set number of realizations and intialize arrays for the 1-sigma bounds on Ahat\n",
    "\n",
    "# loop over amplitudes\n",
    "\n",
    "# for ii in range(N_amplitudes):\n",
    "\n",
    "    # loop over realizations\n",
    "    # for jj in range(N_realizations):\n",
    "    \n",
    "        # simulate data set and get Ahat for new dataset\n",
    "        \n",
    "    # get 1-sigma confidence intervals on Ahat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to get inverse function for the injected amplitude\n",
    "# as a function of the confidence regions on Ahat. We want two\n",
    "# interpolating functions, one for the upper bounds on Ahat and\n",
    "# one for the lower bounds on Ahat. The x values of the interpolating\n",
    "# function should be the lower/upper bounds on Ahat for each value of A,\n",
    "# and the y values should be the values of A themselves\n",
    "\n",
    "\n",
    "# use the interpolation functions and evaluate them at the measured value of Ahat for our original dataset\n",
    "\n",
    "# print the corresponding confidence region on A\n",
    "\n",
    "# plot the confidence belt, measured value of Ahat and a shaded confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.4 Bayesian Credible Interval on Amplitude Parameter</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As talked about above, Bayesian credible intervals use\n",
    "    the likelihood function directly to measure our confidence, or degree of belief\n",
    "    in our measurement of the Amplitude parameter. In practice, this is much less \n",
    "    complicated than constructing frequentist confidence intervals. However in\n",
    "    more complex data analysis problems with many unknown parameters, just constructing\n",
    "    the full likelihood function is very difficult or at least computationally demanding.\n",
    "</p>\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Repeat steps 1-3 from exercise 2.2 to construct the likelihood function for $A$.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Write down an algorithm for computing the Bayesian credible region. You do not have\n",
    "        to write actual code here unless you feel comfortable. Simply think about how you \n",
    "        would set up the problem. <em>Hint:</em> Remember, the desired integral is        \n",
    "        $$\n",
    "        \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$\n",
    "        but it can be re-written as        \n",
    "        $$\n",
    "        \\frac{1-\\alpha}{2} = \\int_{-\\infty}^{\\lambda_1}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$        \n",
    "        and        \n",
    "        $$\n",
    "        \\frac{1+\\alpha}{2} = \\int^{\\infty}_{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use the pre-made function __`confinterval_like()`__ to construct the 1-sigma upper\n",
    "        and lower bounds on $A$.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Plot the likelihood function vs. $A$ along with the 1-sigma upper and lower bounds, \n",
    "        again using the __`plt.plot()`__ and __`plt.axvline()`__ functions.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat steps 1-3 of exercise 2.2\n",
    "\n",
    "# exponentiate likelihood function and compute confidence interval\n",
    "\n",
    "# print upper and lower bounds of credible region\n",
    "\n",
    "# plot the likelihood vs. A in blue along with vertical lines for the upper/lower bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3. Hypothesis Testing: Detecting the Signal</h2>\n",
    "\n",
    "<p class=lead>\n",
    "    Above, when performing parameter estimation we have taken for granted that there was a <em>detectable</em>\n",
    "    signal in the data. Now we turn to the problem of detection, that is, how can we confidently say that there\n",
    "    truly is a signal in the data. In the frequentist framework, detection significance is usually based on the\n",
    "    false alarm probability (sometimes referred to as FAP, I know hilarious right?), that is, the probability \n",
    "    that we would (falsely) claim a detection when that data consists of only noise. Henceforth, we refer to this\n",
    "    noise only case as the <em>null hypothesis</em> and we denote it symbolically as $H_0$. On the other hand we\n",
    "    refer to the case where the data does contain a signal as the <em>signal hypythesis</em> and we denote it symbolically\n",
    "    as $H_1$. In the Bayesian framework, we actually compute the <em>evidence</em> for the signal and null hypotheses and\n",
    "    compare them via the Bayes factor. \n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    In this next section, we will apply these techniques to our simple problem of the sinusoid with unknown amplitude.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3.1 Frequentist Hypothesis Testing</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    The false alarm probability is defined as   \n",
    "    $$\n",
    "    {\\rm FAP} = \\int_{\\hat x}^{\\infty} dx\\,\\, p(x|\\lambda,H_0),\n",
    "    $$    \n",
    "    where $\\hat{x}$ is our measured test statistic, $\\lambda$ is the \n",
    "    parameter of interest, and $p(x|\\lambda,H_0)$ is the pdf of $x$ under the null \n",
    "    hypothesis. The dependence on the actual data, comes in the value of $\\hat{x}$. Essentially what we are doing\n",
    "    is we are testing whether our point extimate of $x$ is consistent with a value of $x$ that we would measure if\n",
    "    the data contains only noise. So to claim a detection, we want to reject the null hypothesis, that is, we want \n",
    "    $\\hat{x}$ to be <em>inconsistent</em> with the distribution of $x$ under the null hypothesis and the false alarm\n",
    "    probability quantifies that inconsistency.\n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    The value of the false alarm probability that one requires for detection is completely problem dependent. In some\n",
    "    cases we would be ok with a value of 5% (i.e., we make a false detection 5% of the time). For example, many of the\n",
    "    social sciences use this value. However, for our purposes we want to be much more confident so we will require \n",
    "    $FAP < 10^{-4}$ for a detection, that is, there is a 1 in 10,000 chance that we have made a false detection. It is\n",
    "    very important to note here that just because we can rule out the null hypothesis with $1-FAP$ confidence, does \n",
    "    <em>not</em> mean that the signal hypothesis is true with $1-FAP$ confidence. In fact, frequentist detection methods \n",
    "    make no statetments about our confidence in the signal model itself, only that a measurement made under the signal \n",
    "    hypothesis is inconsistent with the null hypothesis at the $1-FAP$ level.\n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    Now we want to set the value of $\\hat{x}$ that would be required for us to have $FAP=10^{-4}$. \n",
    "</p>\n",
    "\n",
    "<ol class=lead>    \n",
    "    <li>\n",
    "        First, we want to construct our detection statistic. The <a href=\"http://en.wikipedia.org/wiki/Neyman–Pearson_lemma\">\n",
    "        Neyman-Pearson lemma</a> states that the optimal detection statistic is the likelihood ratio. Use our expression for\n",
    "        the log-likelihood ratio and $\\hat{A}$ from exercise 2.1 to construct the maximum likelihood ratio. <em>Hint:</em>\n",
    "        you should get        \n",
    "        $$\n",
    "        \\log \\Lambda_{\\rm max} = \\frac{1}{2}\\frac{(d|\\tilde s)^2}{(\\tilde s|\\tilde s)}\n",
    "        $$\n",
    "    </li>    \n",
    "    <li>\n",
    "       As we did for the likelihood ratio. Write a function that reads in the data, the time samples, the frequency \n",
    "       of the sine wave and the standard deviation of the noise and returns the maximized log-likelihood ratio.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Now we want to construct the pdf of the maximum log-likelihood ratio under the null hypothesis. As we did\n",
    "        in exercise 2.3, we want to loop over many realizations of data and compute the max log-likelihood ratio,\n",
    "        only this time we simulate only noise. Simulate 100000 realizations of data and store the maximum log-likelihood\n",
    "        values in an array and histogram the results with 50 bins using the __`plt.hist()`__ function.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Now that we have the distribution $p(x|\\lambda,H_0)$, we want to determine the value of $\\hat{x}=\\hat{x}_{\\rm thresh}$\n",
    "        that will give a false alarm probability of $10^{-4}$. We have already build all of the machinery we require to determine\n",
    "        $\\hat{x}_{\\rm thresh}$. <em>Hint:</em> If we re-write the FAP integral, we can use the __`confinterval()`__ function\n",
    "        with the argument onesided=True. Please ask for help if you are stuck here.\n",
    "        <br/>\n",
    "        <span style='color:red'>Expert level: This distribution can be computed analytically. Do you know what it is?</span>\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use our original data with a signal to determine if we have made a detection.\n",
    "    </li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum log likelihood function. Fill in function\n",
    "def maxLogLikelihood(data, t, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of realizations and initialize max log-likelihood array\n",
    "\n",
    "# loop over realizations\n",
    "        \n",
    "    # simulate data and calculate maximum log-likelihood value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of maximum log-likelihood values, use log=True to more clearly see the tail\n",
    "\n",
    "# compute threshold value on the maximum log-likelihood ratio\n",
    "\n",
    "# compute maximum log-likelihood ratio and determine if we have made a detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3.2 Bayesian Hypothesis Testing</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    When considering Bayesian parameter estimation, we used Bayes' theorem for a <em>single</em> hypothesis. We now use the \n",
    "    more general form of Bayes theorem:    \n",
    "    $$\n",
    "    p(\\lambda,H|d) = \\frac{p(d|\\lambda,H)p(\\lambda,H)}{p(d)},\n",
    "    $$\n",
    "    where $p(\\lambda,H|d)$ is the probability of the joint posterior of parameters and the hypotheses, $p(d|\\lambda,H)$ is the likelihood of the data given a set of parameters and hypotheses, $p(\\lambda,H)=p(\\lambda|H)p(H)$ is the prior probability on the parameters and hypotheses and $p(d)$ is the probability of the data itself, also known as the evidence. Here we are interested in the evidence:    \n",
    "    $$\n",
    "    p(d)=\\sum_i\\int d\\lambda_i\\,p(d|\\lambda_i,H_i)p(\\lambda_i|H_i)p(H_i)= \\sum_i p(d|H_i)p(H_i)\n",
    "    $$\n",
    "    which is the sum of the marginal likelihoods for different Hypotheses. In some cases, there may be many possible hypotheses, but it is\n",
    "    always possible to compare different ones. In our case we want to <em>directly</em> calculate the evidence for hypotheses, $H_1$ and $H_0$.\n",
    "    The bayes factor is defined as the ratio of two marginal likelihoods    \n",
    "    $$\n",
    "    B_{10} = \\frac{p(d|H_1)}{p(d|H_0)}.\n",
    "    $$    \n",
    "    In large parameter spaces, this is notoriously hard to calculate; however, in our case it is trivial since our null \n",
    "    hypothesis model has no parameters and our signal model only has 1 parameter we have    \n",
    "    $$\n",
    "    B_{10}=\\int dA\\,\\, \\Lambda(A|d)p(A)\n",
    "    $$    \n",
    "    where $\\Lambda(A|d)$ is just the likelihood ratio defined above and $p(A)$ is the prior on $A$, which we take to be constant. \n",
    "    For our calculation, we will need to include the prior even though it is a constant. You can analytically find the normalizing\n",
    "    constant by integrating $p(A)=C$ over the range of $A$ (which is $[0,1\\times 10^{-19}]$ if using the same as exercise 2.2) and \n",
    "    setting it equal to 1. That is, we require    \n",
    "    $$\n",
    "    \\int dA\\,\\, p(A) = 1\n",
    "    $$\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Loop over 1000 amplitude values and compute the log likelihood just like in exercise 2.2. Once you have the log-likelihood ratio \n",
    "        at each amplitude value you can exponentiate it to obtain the likelihood itself.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use the likelihood values to compute the bayes factor. Remember we can just use a block integral        \n",
    "        $$\n",
    "        \\int dA\\,\\, \\Lambda(A|d) \\approx \\sum_i\\Lambda(A_i|d) \\Delta A.\n",
    "        $$        \n",
    "        The value of the Bayes factor that is normally considered decisive evidence for model $H_1$ over model $H_0$ is 100. This \n",
    "        value comes from the so-called <a href=\"http://en.wikipedia.org/wiki/Bayes_factor#Interpretation\">Jeffreys' scale</a>. For\n",
    "        our purposes here we will follow this scale but in real data analysis applications it is an open problem as to which value \n",
    "        one would truly consider decisive, say for claiming the detection of GWs. One could implement a hybrid frequentist-Bayesian\n",
    "        method where the Bayes factor is used as our test statistic and we can set corresponding FAPs and thresholds for detection.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat exercise 1.2.2 steps 1-3\n",
    "\n",
    "\n",
    "# compute bayes factor with blick integral being sure to normalize prior\n",
    "\n",
    "# print Bayes factor. Is it bigger than 100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.3.2.1 Occam Penalty</h4>\n",
    "\n",
    "<p class=lead>\n",
    "    In hypothesis testing we generally want to choose the simplest model if it provides a good fit to the data. In essence, \n",
    "    we need to weigh the \"goodnes of fit\" versus the simplicity of the model. For example, we could use a free parameter for \n",
    "    every data point, then our model would fit the data perfectly but this model would have hundreds of parameters, versus just\n",
    "    one for our sinusoid model. The bayes factor automatically incorporates this parsimony through the integration of the posterior,\n",
    "    which contains our prior distribution. \n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        We can see this automatic parsimony by choosing a larger prior range on $A$ in the above calculation of the Bayes factor. \n",
    "        Repeat the calculation but now create the amplitude vector going up to $5\\times 10^{-19}$. <em>Hint:</em> Remember to \n",
    "        recalculate the normalization of the prior.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above exercise with different bounds on the Amplitude vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data analysis of unkown sinusoid in unknown white noise\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will perform a similar Bayesian Analysis to the one above but we will be expanding the\n",
    "    number of free parameters from a single unknown amplitude to an unknown, amplitude, frequency, \n",
    "    and phase of the sinusoid plus an unknown white noise level. This will allow us to introduce\n",
    "    multi-parameter likelihoods and Markov Chain Monte-Carlo. Real GW analyses usually have tens to\n",
    "    hundreds of free parameters; however, for this exercise 4 free parameters will give you a feel for\n",
    "    how to run more complex models.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, x, t, pmin=[-22, -5, 0, -22], \n",
    "                 pmax=[-18, -2, 2*np.pi, -18]):\n",
    "        \"\"\"\n",
    "        Model class contains calls to log-likelihood and\n",
    "        log-prior functions. All calls to log-likelihood\n",
    "        and log-prior functions take in parameter array\n",
    "        in the order:\n",
    "        \n",
    "        0: log10 Amplitude [s]\n",
    "        1: log10 frequency [Hz]\n",
    "        2: phase [radian]\n",
    "        3: log10 sigma [s]\n",
    "        \n",
    "        :param x: Data vector\n",
    "        :param t: Times at which data is taken\n",
    "        :param pmin: Minimum values of model parameters\n",
    "        :param pmax: Maximum values of model parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.t = t  \n",
    "        self.pmin = np.array(pmin)\n",
    "        self.pmax = np.array(pmax)\n",
    "    \n",
    "    def get_loglike(self, pars):\n",
    "        \n",
    "        # construct signal from parameter vector pars\n",
    "        \n",
    "        # construct the gaussian log-likelihood including the\n",
    "        # log determinant piece\n",
    "    \n",
    "        return loglike\n",
    "    \n",
    "    def get_logprior(self, pars):\n",
    "        \n",
    "        # check to see if all parameters are > pmin and < pmax\n",
    "        # and return prior (what should the prior value be for uniform priors?)\n",
    "        # return -np.inf otherwise\n",
    "\n",
    "        return prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set up data and model\n",
    "\n",
    "<p class='lead'>\n",
    "    Set up a data set exactly as above in section 1.1 but with phase = 1.5.\n",
    "    You can do this by using the ```phi``` keyword argument to ```wu.simData```\n",
    "    and ```wu.signal```.\n",
    "    \n",
    "    As you go through the next steps in sampling you can simulated different data\n",
    "    sets with different parameters just to get a feel for things.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define frequency, Amplitude, phase, timeseries and sigma\n",
    "\n",
    "# call function to simulate data\n",
    "\n",
    "# call wu.signal function to simulate sine wave individually\n",
    "\n",
    "# plot data in blue with signal in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup model object by calling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MCMC Sampling\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will use a simple MCMC sampler to explore this 4-d parameter\n",
    "    space. We have already provided the sampler via ```smcmc.SimplerMCMC```,\n",
    "    but you can explore it more [here](https://github.com/vhaasteren/cit-busyweek/blob/master/day1/day1_solutions.ipynb).\n",
    "</p>\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Draw initial search parameters from the prior using ```np.random.uniform```</li>\n",
    "    <li> Set jump ```sigmas``` to 0.05</li>\n",
    "    <li> Initialize sampler with loglike and logprior functions from your model object.</li>\n",
    "    <li> Run sampler for 100000 steps. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define initial parameters with np.random.uniform\n",
    "\n",
    "# define number of iterations and sigmas\n",
    "\n",
    "# setup sampler with smcmc.SimpleMCMC\n",
    "\n",
    "# sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Visualizing sampler output\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will perform some standard visualizations for analyzing MCMC output.\n",
    "    These plots are criticial to checking convergence of the MCMC and for visualizing\n",
    "    parameter covariances.   \n",
    "</p>\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Set burn-in to be 25% of chain length</li>\n",
    "    <li> Plot the log posterior, ```sampler.lnprob``` vs iteration number. \n",
    "    This should be stable and not growing with iteration number. </li>\n",
    "    <li> Plot the parameter samples vs. iteration number for each of the 4 parameters. Again these should be stable and sampling around a centeral value.</li>\n",
    "    <li> Use the ```corner``` function to make a triangle plot of the 4 search parameters. Also, plot the injected parameters and the 1-sigma quantiles. See [here](https://github.com/dfm/corner.py) for help.</li>\n",
    "    <li> What can you say about the results of this run. Are the injections consistent with the posterior? What parameters are correlated?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set burn-in to be 25% of chain\n",
    "\n",
    "# plot log-posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of parameter labels\n",
    "pars = ['log-Amplitude', 'log-frequency', 'phase', 'log-sigma']\n",
    "\n",
    "# make list of injected parameters\n",
    "#inj = [np.log10(A), np.log10(freq), phase, np.log10(sigma)]\n",
    "\n",
    "# plot chain traces for 4 parameters\n",
    "#plt.figure(figsize=(10,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot triangle plot using corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Computing Bayesian Evidence via Nested Sampling\n",
    "\n",
    "<p class='lead'>\n",
    "    Here we will be computing the Bayes factor between the signal model\n",
    "    (i.e. signal + noise) and the noise model (i.e. signal=0 + noise). \n",
    "    Previously computing Bayes factor was easy because we simply could\n",
    "    integrate a 1-d function. Now, our posterior is 4-d and the integral\n",
    "    becomes much harder. There are several ways of computing the Bayesian \n",
    "    Evidence such as Nested Sampling, Reversible Jump MCMC, Thermodynamic \n",
    "    integration and others ([see this paper for a good review](http://arxiv.org/abs/0704.1808)).\n",
    "    <br><br>\n",
    "    Here we will be using a nested sampling implementation called [Nestle](http://kbarbary.github.io/nestle/).\n",
    "    We are using this mostly for simplicity as nested sampling is one of the easiest methods for\n",
    "    computing the Bayesian evidence in relatively small parameter spaces (i.e. < 50 parameters). \n",
    "    <br><br>\n",
    "    In the following section, we will modify the model class and compute the Bayes factor!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Modify Model class for Nestle and model selection\n",
    "\n",
    "<p class='lead'>\n",
    "    To run Nestle and to run our noise only model, we will have to\n",
    "    make some modifications to the Model class.\n",
    "    \n",
    "    <ol class=lead>\n",
    "        <li>Add extra option to initialize Model class with a specifice model. \n",
    "        full=signal+noise and null=noise only</li>\n",
    "        <li> Add an if statement to ```get_loglike``` to check for the model\n",
    "        and return the correct likelihood.</li>\n",
    "        <li>Define an extra function ```get_prior_transform``` to compute the prior\n",
    "        transform found [here](http://kbarbary.github.io/nestle/prior.html)</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, x, t, pmin=[-22, -5, 0, -22], \n",
    "                 pmax=[-18, -2, 2*np.pi, -18], model='full'):\n",
    "        \"\"\"\n",
    "        Model class contains calls to log-likelihood and\n",
    "        log-prior functions. All calls to log-likelihood\n",
    "        and log-prior functions take in parameter array\n",
    "        in the order:\n",
    "        \n",
    "        0: log10 Amplitude [s]\n",
    "        1: log10 frequency [Hz]\n",
    "        2: phase [radian]\n",
    "        3: log10 sigma [s]\n",
    "        \n",
    "        if using full model and \n",
    "        \n",
    "        0: log10 sigma[s]\n",
    "        \n",
    "        if using null model\n",
    "        \n",
    "        :param x: Data vector\n",
    "        :param t: Times at which data is taken\n",
    "        :param pmin: Minimum values of model parameters\n",
    "        :param pmax: Maximum values of model parameters\n",
    "        :param model: Model type [full, null]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.t = t  \n",
    "        self.pmin = np.array(pmin)\n",
    "        self.pmax = np.array(pmax)\n",
    "        self.model = model\n",
    "    \n",
    "    def get_loglike(self, pars):\n",
    "        \n",
    "        # is self.model == 'full':\n",
    "            # do what you did before\n",
    "        # elif self.model == 'null':\n",
    "            # return noise only likelihood\n",
    "    \n",
    "        return loglike\n",
    "    \n",
    "    def get_logprior(self, pars):\n",
    "        \n",
    "        # same prior as above\n",
    "\n",
    "        return prior\n",
    "    \n",
    "    def get_prior_transform(self, cube):\n",
    "        \n",
    "        # return transformed prior as in the reference above\n",
    "        \n",
    "        return ptrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Run Nested sampling on signal + noise model\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Initialize full model (i.e. just like before)</li>\n",
    "    <li> Run Nestle with ```nestle.sample``` using 300 live points and ```dlogz```=0.1</li>\n",
    "    <li> Print the summary with ```print result.summary()```</li>\n",
    "    <li> Inspect posterior with ```corner``` as above. However, this time you will\n",
    "    have to add a a few extra arguments such as ```weights=result.weights``` and \n",
    "    ```range=[0.997, 0.997, 0.997, 0.997]```. The weights are due to the fact that Nested\n",
    "    Sampling collects samples different than MCMC and the range is so that we zoom in on the \n",
    "    high probability regions.</li>\n",
    "    <li>Do these posteriors seem consistent with the MCMC?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize noise + signal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set ndim = 4\n",
    "# return nestle.sample result in fullresult\n",
    "# print fullresult.summar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call corner with modifications above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Run Nestle on null model\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Initilize the null model. You will have to give the initialization\n",
    "    ```pmin=[-9], pmax=[-6], model='null'``` so that it knows to use the noise\n",
    "    only model.</li>\n",
    "    <li> Run Nestle with the same settings (remember to set ```ndim```=1 though)</li>\n",
    "    <li>Compute the log Bayes factor by computing the difference in logz between the two\n",
    "    models.</li>\n",
    "    <li>How significant is the result based on the Bayes factor interpretations mentioned in \n",
    "    problem 1.3.2?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize noise only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run nestle as above but set ndim = 1\n",
    "# return result in nullresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute Bayes factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Other injections\n",
    "\n",
    "<p class='lead'>\n",
    "    Repeat all of this section with injections with sinusoid amplitudes of \n",
    "    5, 15, and 30 ns. You can just change the amplitude above and re-evaluate the cells.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
